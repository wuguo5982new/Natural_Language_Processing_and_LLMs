{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts-of-Speech Tagging (POS)\n",
    "\n",
    "This project we build part-of-speech (POS) tagging for dataset, create and compute transition matrix and transmission matrix in Hidden Markov Models (HMM), implement Viterbi Algorithm through Forward and Backward process (see Coursera). Then finally predict testing word.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training corpus\n",
      "['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n', 'of\\tIN\\n', '``\\t``\\n', 'The\\tDT\\n', 'Misanthrope\\tNN\\n', \"''\\t''\\n\"] \n",
      "\n",
      "test corpus\n",
      "['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n', 'be\\tVB\\n', 'taken\\tVBN\\n', 'from\\tIN\\n', 'several\\tJJ\\n', 'vantage\\tNN\\n'] \n",
      "\n",
      "the vocabulary list\n",
      "['!', '#', '$', '%', '&', \"'\", \"''\", \"'40s\", \"'60s\", \"'70s\"] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training corpus\n",
    "with open(\"WSJ_02-21.pos\", 'r') as f:\n",
    "    training_corpus = f.readlines()\n",
    "print(f\"training corpus\")\n",
    "print(training_corpus[0:10], '\\n')\n",
    "\n",
    "# Test corpus\n",
    "with open(\"WSJ_24.pos\", 'r') as f:\n",
    "    y = f.readlines()\n",
    "print(\"test corpus\")\n",
    "print(y[0:10], '\\n')\n",
    "\n",
    "with open(\"hmm_vocab.txt\", 'r') as f:\n",
    "    voc_l = f.read().split('\\n')\n",
    "print(\"the vocabulary list\")\n",
    "print(voc_l[0:10], '\\n')\n",
    "\n",
    "# Vocabulary dictionary\n",
    "vocab = {} \n",
    "for i, word in enumerate(sorted(voc_l)): \n",
    "    vocab[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(vocab, data_fp):\n",
    "    orig = []\n",
    "    prep = []\n",
    "    with open(data_fp, \"r\") as data_file:\n",
    "        for cnt, word in enumerate(data_file):\n",
    "            if not word.split():\n",
    "                orig.append(word.strip())\n",
    "                word = \"--n--\"\n",
    "                prep.append(word)\n",
    "                continue\n",
    "            # For unknown words\n",
    "            elif word.strip() not in vocab:\n",
    "                orig.append(word.strip())\n",
    "                word = assign_unk(word)\n",
    "                prep.append(word)\n",
    "                continue\n",
    "            else:\n",
    "                orig.append(word.strip())\n",
    "                prep.append(word.strip())\n",
    "    assert(len(orig) == len(open(data_fp, \"r\").readlines()))\n",
    "    assert(len(prep) == len(open(data_fp, \"r\").readlines()))\n",
    "    return orig, prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punct = set(string.punctuation)\n",
    "noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
    "verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
    "adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
    "\n",
    "def assign_unk(tok):\n",
    "    if any(char.isdigit() for char in tok): # Digits\n",
    "        return \"--unk_digit--\"\n",
    "    elif any(char in punct for char in tok): # Punctuation    \n",
    "        return \"--unk_punct--\"    \n",
    "    elif any(char.isupper() for char in tok): # Upper-case\n",
    "        return \"--unk_upper--\"    \n",
    "    elif any(tok.endswith(suffix) for suffix in noun_suffix): # Nouns\n",
    "        return \"--unk_noun--\"    \n",
    "    elif any(tok.endswith(suffix) for suffix in verb_suffix): # Verbs\n",
    "        return \"--unk_verb--\"    \n",
    "    elif any(tok.endswith(suffix) for suffix in adj_suffix):  # Adjectives\n",
    "        return \"--unk_adj--\"    \n",
    "    elif any(tok.endswith(suffix) for suffix in adv_suffix):  # Adverbs\n",
    "        return \"--unk_adv--\"\n",
    "    return \"--unk--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the preprocessed test corpus:  34199\n",
      "This is a sample of the test_corpus: \n",
      "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--', 'points', 'this', 'week', ',', 'with', 'readings', 'on', 'trade', ',', 'output']\n"
     ]
    }
   ],
   "source": [
    "# corpus without tags, preprocessed\n",
    "_, prep = preprocess(vocab, \"test.words\")     \n",
    "\n",
    "print('The length of the preprocessed test corpus: ', len(prep))\n",
    "print('This is a sample of the test_corpus: ')\n",
    "print(prep[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`emission_counts`: maps (tag, word) to the number of times it happened. \n",
    "`transition_counts`: maps (prev_tag, tag) to the number of times it has appeared. \n",
    "`tag_counts`: maps (tag) to the number of times it has occured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tag(line, vocab): \n",
    "    if not line.split():\n",
    "        word = \"--n--\"\n",
    "        tag = \"--s--\"\n",
    "        return word, tag\n",
    "    else:\n",
    "        word, tag = line.split()\n",
    "        if word not in vocab: \n",
    "            # Handle unknown words\n",
    "            word = assign_unk(word)\n",
    "        return word, tag\n",
    "    return None \n",
    "\n",
    "def create_dictionaries(training_corpus, vocab):\n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)    \n",
    "    prev_tag = '--s--' \n",
    "    \n",
    "    i = 0     \n",
    "    for word_tag in training_corpus:\n",
    "        i += 1       \n",
    "        word, tag = get_word_tag(word_tag,vocab)        \n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        emission_counts[(tag, word)] += 1\n",
    "        tag_counts[tag] += 1\n",
    "        prev_tag = tag        \n",
    "    return emission_counts, transition_counts, tag_counts\n",
    "\n",
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POS tags (number of 'states'): 46\n",
      "View these POS tags (states)\n",
      "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "# get all the POS states\n",
    "states = sorted(tag_counts.keys())\n",
    "print(f\"Number of POS tags (number of 'states'): {len(states)}\")\n",
    "print(\"View these POS tags (states)\")\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition examples: \n",
      "(('--s--', 'IN'), 5050)\n",
      "(('IN', 'DT'), 32364)\n",
      "(('DT', 'NNP'), 9044)\n",
      "emission examples: \n",
      "(('DT', 'any'), 721)\n",
      "(('NN', 'decrease'), 7)\n",
      "(('NN', 'insider-trading'), 5)\n",
      "ambiguous word example: \n",
      "('RB', 'back') 304\n",
      "('VB', 'back') 20\n",
      "('RP', 'back') 84\n",
      "('JJ', 'back') 25\n",
      "('NN', 'back') 29\n",
      "('VBP', 'back') 4\n"
     ]
    }
   ],
   "source": [
    "print(\"transition examples: \")\n",
    "for ex in list(transition_counts.items())[:3]:\n",
    "    print(ex)\n",
    "print(\"emission examples: \")\n",
    "for ex in list(emission_counts.items())[200:203]:\n",
    "    print (ex)\n",
    "print(\"ambiguous word example: \")\n",
    "for tup,cnt in emission_counts.items():\n",
    "    if tup[1] == 'back': print (tup, cnt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction using predict_pos is 0.8889\n"
     ]
    }
   ],
   "source": [
    "def predict_pos(prep, y, emission_counts, vocab, states):\n",
    "    num_correct = 0\n",
    "    all_words = set(emission_counts.keys())\n",
    "    total = len(y)\n",
    "    for word, y_tup in zip(prep, y): \n",
    "        y_tup_l = y_tup.split()\n",
    "        if len(y_tup_l) == 2:\n",
    "            true_label = y_tup_l[1]\n",
    "        else:\n",
    "            continue    \n",
    "        count_final = 0\n",
    "        pos_final = ''\n",
    "\n",
    "        if word in vocab:\n",
    "            for pos in states:\n",
    "                key = (pos,word)\n",
    "                if key in emission_counts: # complete this line\n",
    "                    count = emission_counts[key]\n",
    "                    if count>count_final: # complete this line\n",
    "                        count_final = count\n",
    "                        pos_final = pos\n",
    "            if pos_final == true_label:\n",
    "                num_correct += 1            \n",
    "    accuracy = num_correct / total    \n",
    "    return accuracy\n",
    "\n",
    "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n",
    "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov Model utilizes a transition  matrix,  `A`.    \n",
    "A Hidden Markov Model adds an observation or emission matrix `B` when we are in a particular state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    num_tags = len(all_tags)    \n",
    "    A = np.zeros((num_tags,num_tags))\n",
    "    trans_keys = set(transition_counts.keys())\n",
    "    for i in range(num_tags):\n",
    "        for j in range(num_tags):\n",
    "            count = 0\n",
    "            key = (all_tags[i],all_tags[j])\n",
    "            if key in transition_counts: #complete this line\n",
    "                count = transition_counts[key]\n",
    "            count_prev_tag = tag_counts[all_tags[i]]\n",
    "            A[i,j] = (count + alpha) / (count_prev_tag + alpha*num_tags)   \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A at row 0, col 0: 0.000007040\n",
      "A at row 3, col 1: 0.1691\n",
      "              RBS            RP           SYM        TO            UH\n",
      "RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\n",
      "RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\n",
      "SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\n",
      "TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\n",
      "UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "for i in range(46):\n",
    "    tag_counts.pop(i,None)\n",
    "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "# Testing your function\n",
    "print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n",
    "print(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n",
    "\n",
    "#print(\"View a subset of transition matrix A\")\n",
    "A_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\n",
    "print(A_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    num_tags = len(tag_counts)\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    num_words = len(vocab)\n",
    "    B = np.zeros((num_tags, num_words))\n",
    "    emis_keys = set(list(emission_counts.keys()))\n",
    "    for i in range(num_tags): \n",
    "        for j in range(num_words): \n",
    "            count = 0\n",
    "            key = (all_tags[i],vocab[j])\n",
    "            if key in emission_counts.keys(): \n",
    "                count = emission_counts[key]\n",
    "            count_tag = tag_counts[all_tags[i]]\n",
    "            B[i,j] = (count + alpha) / (count_tag+ alpha*num_words)\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Matrix position at row 0, column 0: 0.000006032\n",
      "View Matrix position at row 3, column 1: 0.000000720\n",
      "              725      adroitly     engineers      promoted       synergy\n",
      "CD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08\n",
      "NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\n",
      "NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\n",
      "VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\n",
      "RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\n",
      "RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07\n"
     ]
    }
   ],
   "source": [
    "for i in range(46\\):\n",
    "    tag_counts.pop(i,None)\n",
    "\n",
    "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
    "print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n",
    "print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n",
    "\n",
    "cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n",
    "cols = [vocab[a] for a in cidx]\n",
    "rvals =['CD','NN','NNS', 'VB','RB','RP']\n",
    "rows = [states.index(a) for a in rvals]\n",
    "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
    "print(B_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,0]: -22.6098\n",
      "best_paths[2,3]: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
    "    num_tags = len(tag_counts)\n",
    "    best_probs = np.zeros((num_tags, len(corpus)))\n",
    "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
    "    s_idx = states.index(\"--s--\")\n",
    "    for i in range(num_tags):         \n",
    "        if A[s_idx,i] == 0: \n",
    "            best_probs[i,0] = float('-inf')\n",
    "        else:\n",
    "            best_probs[i,0] = math.log(A[s_idx,i]) + math.log(B[i,vocab[corpus[0]]] )\n",
    "    return best_probs, best_paths\n",
    "\n",
    "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)\n",
    "print(f\"best_probs[0,0]: {best_probs[0,0]:.4f}\") \n",
    "print(f\"best_paths[2,3]: {best_paths[2,3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words processed:     5000\n",
      "Words processed:    10000\n",
      "Words processed:    15000\n",
      "Words processed:    20000\n",
      "Words processed:    25000\n",
      "Words processed:    30000\n",
      "best_probs[0,1]: -24.7822\n",
      "best_probs[0,4]: -49.5601\n"
     ]
    }
   ],
   "source": [
    "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n",
    "    num_tags = best_probs.shape[0]\n",
    "    for i in range(1, len(test_corpus)): \n",
    "        if i % 5000 == 0:\n",
    "            print(\"Words processed: {:>8}\".format(i))\n",
    "        for j in range(num_tags): # complete this line\n",
    "            best_prob_i =  float(\"-inf\")\n",
    "            best_path_i = None\n",
    "            for k in range(num_tags): # complete this line\n",
    "                prob = best_probs[k,i-1]+math.log(A[k,j]) +math.log(B[j,vocab[test_corpus[i]]])\n",
    "                if prob > best_prob_i:\n",
    "                    best_prob_i = prob\n",
    "                    best_path_i = k\n",
    "            best_probs[j,i] = best_prob_i\n",
    "            best_paths[j,i] = best_path_i\n",
    "    return best_probs, best_paths\n",
    "\n",
    "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)\n",
    "# Test this function \n",
    "print(f\"best_probs[0,1]: {best_probs[0,1]:.4f}\") \n",
    "print(f\"best_probs[0,4]: {best_probs[0,4]:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for pred[-7:m-1] is: \n",
      " ['see', 'them', 'here', 'with', 'us', '.'] \n",
      " ['VB', 'PRP', 'RB', 'IN', 'PRP', '.'] \n",
      "\n",
      "The prediction for pred[0:8] is: \n",
      " ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN'] \n",
      " ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken']\n",
      "The third word is: temperature\n",
      "Your prediction is: NN\n",
      "Your corresponding label y is:  temperature\tNN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "    m = best_paths.shape[1] \n",
    "    z = [None] * m\n",
    "    num_tags = best_probs.shape[0]\n",
    "    best_prob_for_last_word = float('-inf')\n",
    "\n",
    "    pred = [None] * m\n",
    "    for k in range(num_tags): # complete this line\n",
    "        if best_probs[k, m - 1]>best_prob_for_last_word: \n",
    "            best_prob_for_last_word = best_probs[k, m - 1]\n",
    "            z[m - 1]=k\n",
    "    pred[m - 1] = states[z[m - 1]]\n",
    "\n",
    "    for i in range(m-1, -1, -1): # complete this line\n",
    "        pos_tag_for_word_i = z[i]\n",
    "        z[i - 1] = best_paths[pos_tag_for_word_i,i]\n",
    "        pred[i - 1] = states[z[i - 1]]\n",
    "    return pred\n",
    "\n",
    "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
    "m=len(pred)\n",
    "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
    "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])\n",
    "\n",
    "# Predicting on a data set\n",
    "print('The third word is:', prep[3])\n",
    "print('Your prediction is:', pred[3])\n",
    "print('Your corresponding label y is: ', y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Viterbi algorithm is 0.9531\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(pred, y):\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    for prediction, y in zip(pred, y):\n",
    "        word_tag_tuple = y.split()\n",
    "        if len(word_tag_tuple)!=2:\n",
    "            continue \n",
    "        word, tag = word_tag_tuple\n",
    "        if prediction == tag: # complete this line\n",
    "            num_correct += 1\n",
    "        total += 1\n",
    "    return (num_correct/total)\n",
    "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- https://www.coursera.org/learn/probabilistic-models-in-nlp?\n",
    "- [\"Speech and Language Processing\", Dan Jurafsky and James H. Martin](https://web.stanford.edu/~jurafsky/slp3/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC2-2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
