{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with Transformer-based BERT model\n",
    "\n",
    "We used Transformer-based BERT model for sentiment analysis (movie reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import BertTokenizer, BertModel, TFAutoModel\n",
    "import torch\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train.tsv.zip to .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.28M/1.28M [00:00<00:00, 12.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading test.tsv.zip to .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 494k/494k [00:00<00:00, 56.2MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import zipfile\n",
    "import os\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "for file in ['train.tsv', 'test.tsv']:\n",
    "    api.competition_download_file('sentiment-analysis-on-movie-reviews', f'{file}.zip', path='./')\n",
    "    with zipfile.ZipFile(f'{file}.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./')\n",
    "    os.remove(f'{file}.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76167</th>\n",
       "      <td>76168</td>\n",
       "      <td>3907</td>\n",
       "      <td>unwavering and</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82936</th>\n",
       "      <td>82937</td>\n",
       "      <td>4283</td>\n",
       "      <td>-LRB- Hayek -RRB- throws herself into this dre...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95349</th>\n",
       "      <td>95350</td>\n",
       "      <td>4977</td>\n",
       "      <td>Tautou</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4770</th>\n",
       "      <td>4771</td>\n",
       "      <td>182</td>\n",
       "      <td>are powerful and moving without stooping to ba...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PhraseId  SentenceId  \\\n",
       "76167     76168        3907   \n",
       "82936     82937        4283   \n",
       "95349     95350        4977   \n",
       "4770       4771         182   \n",
       "\n",
       "                                                  Phrase  Sentiment  \n",
       "76167                                     unwavering and          2  \n",
       "82936  -LRB- Hayek -RRB- throws herself into this dre...          4  \n",
       "95349                                             Tautou          2  \n",
       "4770   are powerful and moving without stooping to ba...          4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.tsv', sep='\\t')\n",
    "print(df.shape)\n",
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    79582\n",
       "3    32927\n",
       "1    27273\n",
       "4     9206\n",
       "0     7072\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff=[len(i.split(\" \")) for i in df.Phrase[:10]]         \n",
    "max(dff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Create two input tensor (input IDs and attention mask)\n",
    "import numpy as np\n",
    "seq_len = 40               # Sequence length of tokenized sequences for BERT\n",
    "num_samples = len(df)        # Number of samples in our dataset is 156060   \n",
    "\n",
    "# Initialize \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# Tokenize \n",
    "tokens = tokenizer(df['Phrase'].tolist(), max_length=seq_len, truncation=True,\n",
    "                   padding=True, add_special_tokens=True,\n",
    "                   return_tensors='np')   # Return Numpy tensors\n",
    "tokens.keys()      # Three numpy arrays: input_ids, token_type_ids and attention_mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,   138,  1326,  1104, 13936, 25265, 16913, 15107,  1103,\n",
       "         8050,  2553,  1115,  1184,  1110,  1363,  1111,  1103, 20398,\n",
       "         1110,  1145,  1363,  1111,  1103,   176,  9900,   117,  1199,\n",
       "         1104,  1134,  5411,  1821, 14225,  1133,  3839,  1104,  1134,\n",
       "         7919,  1106,  1277,   102],\n",
       "       [  101,   138,  1326,  1104, 13936, 25265, 16913, 15107,  1103,\n",
       "         8050,  2553,  1115,  1184,  1110,  1363,  1111,  1103, 20398,\n",
       "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0],\n",
       "       [  101,   138,  1326,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0],\n",
       "       [  101,   138,   102,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0],\n",
       "       [  101,  1326,   102,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['attention_mask'][:5]            # 1 means the value, 0 means none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as binary files (Numpy)\n",
    "with open('movie-xids.npy', 'wb') as f:\n",
    "    np.save(f, tokens['input_ids'])\n",
    "with open('movie-xmask.npy', 'wb') as f:\n",
    "    np.save(f, tokens['attention_mask'])    \n",
    "# Free memory.\n",
    "del tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 5)\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoded labels array\n",
    "arr = df['Sentiment'].values\n",
    "\n",
    "labels = np.zeros((num_samples, arr.max()+1))\n",
    "print(labels.shape)\n",
    "\n",
    "labels[np.arange(num_samples), arr] = 1\n",
    "# labels = labels.to(device)\n",
    "print(labels)\n",
    "with open('movie-labels.npy', 'wb') as f:\n",
    "    np.save(f, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "with open('movie-xids.npy', 'rb') as f:\n",
    "    Xids = np.load(f, allow_pickle=True)\n",
    "with open('movie-xmask.npy', 'rb') as f:\n",
    "    Xmask = np.load(f, allow_pickle=True)\n",
    "with open('movie-labels.npy', 'rb') as f:\n",
    "    labels = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(40,), dtype=tf.int32, name=None), TensorSpec(shape=(40,), dtype=tf.int32, name=None), TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create TF dataset \n",
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(40,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(40,), dtype=tf.int32, name=None)}, TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rearange the dataset format\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
    "\n",
    "# Apply map for transformation function.\n",
    "dataset = dataset.map(map_func)\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(64, 40), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(64, 40), dtype=tf.int32, name=None)}, TensorSpec(shape=(64, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "size = int((Xids.shape[0] / batch_size) * ratio)\n",
    "train_ds = dataset.take(size)\n",
    "val_ds = dataset.skip(size)\n",
    "# val_ds.element_spec == train_ds.element_spec\n",
    "\n",
    "tf.data.experimental.save(train_ds, 'train')\n",
    "tf.data.experimental.save(val_ds, 'val')\n",
    "del dataset                   # Fresh memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108310272 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,310,272\n",
      "Trainable params: 108,310,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert = TFAutoModel.from_pretrained('bert-base-cased')\n",
    "bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 40,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         787456      ['bert[0][1]']                   \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 5)            5125        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,102,853\n",
      "Trainable params: 792,581\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Two input layers\n",
    "input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# Access the transformer model\n",
    "embeddings = bert.bert(input_ids, attention_mask=mask)[1]  # [1] final activations\n",
    "# Convert into five classes\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(embeddings)\n",
    "y = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(x)\n",
    "# Initialize \n",
    "model = Model(inputs=[input_ids, mask], outputs=y)\n",
    "model.layers[2].trainable = False     # (optional) freeze bert layer to save time.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sealion\\anaconda3\\envs\\python\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Customized model\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-1, decay=1e-1)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(64, 40), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(64, 40), dtype=tf.int32, name=None)}, TensorSpec(shape=(64, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_spec = ({'input_ids': tf.TensorSpec(shape=(batch_size, seq_len), dtype='int32', name=None),\n",
    "                 'attention_mask': tf.TensorSpec(shape=(batch_size, seq_len), dtype='int32', name=None)},\n",
    "                tf.TensorSpec(shape=(batch_size, 5), dtype=tf.float64, name=None))\n",
    "\n",
    "# load the training and validation sets\n",
    "train_ds = tf.data.experimental.load('train', element_spec=element_spec)\n",
    "val_ds = tf.data.experimental.load('val', element_spec=element_spec)\n",
    "# view format\n",
    "train_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': TensorSpec(shape=(64, 40), dtype=tf.int32, name=None),\n",
       "  'attention_mask': TensorSpec(shape=(64, 40), dtype=tf.int32, name=None)},\n",
       " TensorSpec(shape=(64, 5), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': TensorSpec(shape=(64, 40), dtype=tf.int32, name=None),\n",
       "  'attention_mask': TensorSpec(shape=(64, 40), dtype=tf.int32, name=None)},\n",
       " TensorSpec(shape=(64, 5), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1950/1950 [==============================] - 20910s 11s/step - loss: 3.4426 - accuracy: 0.5331 - val_loss: 1.1204 - val_accuracy: 0.5479\n",
      "Epoch 2/3\n",
      "1950/1950 [==============================] - 16046s 8s/step - loss: 1.0841 - accuracy: 0.5661 - val_loss: 1.0824 - val_accuracy: 0.5622\n",
      "Epoch 3/3\n",
      "1950/1950 [==============================] - 8636s 4s/step - loss: 1.0682 - accuracy: 0.5698 - val_loss: 1.0687 - val_accuracy: 0.5686\n"
     ]
    }
   ],
   "source": [
    "# Train model \n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=3)  # We use simple parameters to save time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 1055). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentiment_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sentiment_model\\assets\n",
      "C:\\Users\\Sealion\\anaconda3\\envs\\python\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    }
   ],
   "source": [
    "model.save('sentiment_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " bert (Custom>TFBertMainLayer)  {'pooler_output': (  108310272   ['input_ids[0][0]',              \n",
      "                                None, 768),                       'attention_mask[0][0]']         \n",
      "                                 'last_hidden_state                                               \n",
      "                                ': (None, 40, 768)}                                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         787456      ['bert[0][1]']                   \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 5)            5125        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,102,853\n",
      "Trainable params: 792,581\n",
      "Non-trainable params: 108,310,272\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('sentiment_model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(text):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    tokens = tokenizer.encode_plus(text, max_length=max_length,\n",
    "                                   truncation=True, padding='max_length',\n",
    "                                   add_special_tokens=True, return_token_type_ids=False,\n",
    "                                   return_tensors='tf')\n",
    "    # Convert int32 tensors to float64\n",
    "    return {'input_ids': tf.cast(tokens['input_ids'], tf.float64),\n",
    "            'attention_mask': tf.cast(tokens['attention_mask'], tf.float64)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02690142 0.08788886 0.56382006 0.27686545 0.04452418]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "max_length=40\n",
    "probs = model.predict(prep_data(\"hello world\"))[0]\n",
    "print(probs)\n",
    "# np.argmax(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine effort .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId  \\\n",
       "0    156061        8545   \n",
       "1    156062        8545   \n",
       "2    156063        8545   \n",
       "3    156064        8545   \n",
       "4    156065        8545   \n",
       "\n",
       "                                                   Phrase  \n",
       "0  An intermittently pleasing but mostly routine effort .  \n",
       "1    An intermittently pleasing but mostly routine effort  \n",
       "2                                                      An  \n",
       "3       intermittently pleasing but mostly routine effort  \n",
       "4              intermittently pleasing but mostly routine  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.read_csv('test.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>156900</td>\n",
       "      <td>8569</td>\n",
       "      <td>A fitfully amusing romp that , if nothing else , will appeal to fans of Malcolm in the Middle and its pubescent star , Frankie Muniz .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>156607</td>\n",
       "      <td>8560</td>\n",
       "      <td>Chilling but uncommercial look into the mind of Jeffrey Dahmer , serial killer .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>156537</td>\n",
       "      <td>8557</td>\n",
       "      <td>For all its failed connections , Divine Secrets of the Ya-Ya Sisterhood is nurturing , in a gauzy , dithering way .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>157602</td>\n",
       "      <td>8594</td>\n",
       "      <td>And when you 're talking about a slapstick comedy , that 's a pretty big problem .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>157290</td>\n",
       "      <td>8582</td>\n",
       "      <td>What Jackson has accomplished here is amazing on a technical level .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      PhraseId  SentenceId  \\\n",
       "839     156900        8569   \n",
       "546     156607        8560   \n",
       "476     156537        8557   \n",
       "1541    157602        8594   \n",
       "1229    157290        8582   \n",
       "\n",
       "                                                                                                                                      Phrase  \\\n",
       "839   A fitfully amusing romp that , if nothing else , will appeal to fans of Malcolm in the Middle and its pubescent star , Frankie Muniz .   \n",
       "546                                                         Chilling but uncommercial look into the mind of Jeffrey Dahmer , serial killer .   \n",
       "476                      For all its failed connections , Divine Secrets of the Ya-Ya Sisterhood is nurturing , in a gauzy , dithering way .   \n",
       "1541                                                      And when you 're talking about a slapstick comedy , that 's a pretty big problem .   \n",
       "1229                                                                    What Jackson has accomplished here is amazing on a technical level .   \n",
       "\n",
       "     Sentiment  \n",
       "839          3  \n",
       "546          2  \n",
       "476          3  \n",
       "1541         1  \n",
       "1229         3  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df.drop_duplicates(subset=['SentenceId'], keep='first')[:50]    # save time [:50]\n",
    "df_test['Sentiment'] = None\n",
    "for i, row in df_test.iterrows():\n",
    "    tokens = prep_data(row['Phrase'])\n",
    "    probs = model.predict(tokens)\n",
    "    pred = np.argmax(probs)\n",
    "    df_test.at[i, 'Sentiment'] = pred\n",
    "df_test.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert = TFAutoModel.from_pretrained('bert-base-cased')\n",
    "input_ids = tf.keras.layers.Input(shape=(max_length,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(max_length,), name='attention_mask', dtype='int32')\n",
    "embeddings = bert.bert(input_ids, attention_mask=mask)[0]  # access final activations with [0]\n",
    "\n",
    "# convert bert embeddings into 5 output classes\n",
    "x = tf.keras.layers.LSTM(32, dropout=.3, recurrent_dropout=.3, return_sequences=True)(embeddings)\n",
    "x = tf.keras.layers.LSTM(16, dropout=.4, recurrent_dropout=.4, return_sequences=False)(x)\n",
    "# normalize\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "# output\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "y = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 40)]         0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 40,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " lstm_10 (LSTM)                 (None, 40, 32)       102528      ['bert[0][0]']                   \n",
      "                                                                                                  \n",
      " lstm_11 (LSTM)                 (None, 16)           3136        ['lstm_10[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 16)          64          ['lstm_11[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           1088        ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 5)            325         ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 108,417,413\n",
      "Trainable params: 107,109\n",
      "Non-trainable params: 108,310,304\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "model2 = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "# (optional) freeze bert layer\n",
    "model2.layers[2].trainable = False\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sealion\\anaconda3\\envs\\python\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=1e-1, decay=1e-1)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "mod  el2.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=({'input_ids': TensorSpec(shape=(64, 40), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(64, 40), dtype=tf.int32, name=None)}, TensorSpec(shape=(64, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_spec = ({'input_ids': tf.TensorSpec(shape=(64, 40), dtype='int32', name=None),\n",
    "                 'attention_mask': tf.TensorSpec(shape=(64, 40), dtype='int32', name=None)},\n",
    "                tf.TensorSpec(shape=(64, 5), dtype=tf.float64, name=None))\n",
    "\n",
    "# load the training and validation sets\n",
    "train_ds = tf.data.experimental.load('train', element_spec=element_spec)\n",
    "val_ds = tf.data.experimental.load('val', element_spec=element_spec)\n",
    "\n",
    "# view the input format\n",
    "train_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(train_ds, validation_data=val_ds, epochs=2)        # Save time\n",
    "model2.save('sentiment_model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acknowledge\n",
    "# References:\n",
    "        1. Kaggle\n",
    "        2. Udemy\n",
    "        3. http://web.stanford.edu/class/cs224n/       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
