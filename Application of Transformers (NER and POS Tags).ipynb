{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf33e80",
   "metadata": {},
   "source": [
    "### Transformers Applications for Tokens (NER/POS tags datasets) \n",
    "1. Created models for prediction on a per token basis (NER/POS tags datasets provided);\n",
    "2. Tokenize words into subwords through Transformers library;\n",
    "3. Pre-trained transformers model (\"distilbert-base-cased\") is faster than BERT;\n",
    "4. To learn structure of datasets and pre-trained model, and study how to make predictions by using transformers models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f24ee4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (C:/Users/Sealion/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be80c2cd882f405b83a9147aca9d1114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n",
       " 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install transformers datasets\n",
    "from datasets import load_dataset\n",
    "data = load_dataset('conll2003')\n",
    "print(data)\n",
    "data['train']\n",
    "data['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35f34e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "    num_rows: 14041\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ae4041b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23757805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = data['train'].features['ner_tags'].feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8026073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15524a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "t = tokenizer(data['train'][idx][\"tokens\"], is_split_into_words=True)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf95c77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'EU',\n",
       " 'rejects',\n",
       " 'German',\n",
       " 'call',\n",
       " 'to',\n",
       " 'boycott',\n",
       " 'British',\n",
       " 'la',\n",
       " '##mb',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a8ee884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "730d7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin2inside = {1:2, 3:4, 5:6, 7:8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d67726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_targets(labels, word_ids):\n",
    "    aligned_labels = []\n",
    "    last_word = None\n",
    "    for word in word_ids:\n",
    "        if word is None:\n",
    "            label = -100\n",
    "        elif word != last_word:\n",
    "            label = labels[word]\n",
    "        else:\n",
    "            label = labels[word]\n",
    "            if label in begin2inside:\n",
    "                label = begin2inside[label]\n",
    "        aligned_labels.append(label)\n",
    "        \n",
    "        last_word = word\n",
    "    return aligned_labels      \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9b88ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test\n",
    "labels = data['train'][idx]['ner_tags']\n",
    "word_ids = t.word_ids()\n",
    "aligned_targets = align_targets(labels, word_ids)\n",
    "aligned_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "470e548e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\\None\n",
      "EU\\B-ORG\n",
      "rejects\\O\n",
      "German\\B-MISC\n",
      "call\\O\n",
      "to\\O\n",
      "boycott\\O\n",
      "British\\B-MISC\n",
      "la\\O\n",
      "##mb\\O\n",
      ".\\O\n",
      "[SEP]\\None\n"
     ]
    }
   ],
   "source": [
    "aligned_labels = [label_names[t] if t>= 0 else None for t in aligned_targets]\n",
    "for x, y in zip(t.tokens(), aligned_labels):\n",
    "    print(f\"{x}\\{y}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bc108b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize both inputs and targets\n",
    "def tokenize_fn(batch):\n",
    "    tokenized_inputs = tokenizer(\n",
    "    batch['tokens'], truncation=True, is_split_into_words=True)\n",
    "    \n",
    "    labels_batch = batch['ner_tags']\n",
    "    aligned_labels_batch = []\n",
    "    for i, labels in enumerate(labels_batch):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        aligned_labels_batch.append(align_targets(labels, word_ids))\n",
    "        \n",
    "    tokenized_inputs['labels'] = aligned_labels_batch\n",
    "    return tokenized_inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cc4a6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0f053db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Sealion\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98\\cache-166759d3ce38d03c.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Sealion\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98\\cache-7a5d616a76ade34a.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = data.map(tokenize_fn, batched=True, remove_columns=data['train'].column_names,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fbccb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddf7aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5078ce4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101,\n",
       "   7270,\n",
       "   22961,\n",
       "   1528,\n",
       "   1840,\n",
       "   1106,\n",
       "   21423,\n",
       "   1418,\n",
       "   2495,\n",
       "   12913,\n",
       "   119,\n",
       "   102],\n",
       "  [101, 1943, 14428, 102]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1]],\n",
       " 'labels': [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100], [-100, 1, 2, -100]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e11fbb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': [101,\n",
       "   7270,\n",
       "   22961,\n",
       "   1528,\n",
       "   1840,\n",
       "   1106,\n",
       "   21423,\n",
       "   1418,\n",
       "   2495,\n",
       "   12913,\n",
       "   119,\n",
       "   102],\n",
       "  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]},\n",
       " {'input_ids': [101, 1943, 14428, 102],\n",
       "  'attention_mask': [1, 1, 1, 1],\n",
       "  'labels': [-100, 1, 2, -100]}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenized_datasets['train'][i] for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac6cdb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
       "        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "batch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\n",
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c217d566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a50a0066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForTokenClassification(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1376855e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101,\n",
       "   7270,\n",
       "   22961,\n",
       "   1528,\n",
       "   1840,\n",
       "   1106,\n",
       "   21423,\n",
       "   1418,\n",
       "   2495,\n",
       "   12913,\n",
       "   119,\n",
       "   102],\n",
       "  [101, 1943, 14428, 102]],\n",
       " 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1]],\n",
       " 'labels': [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100], [-100, 1, 2, -100]]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d077c369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': [101,\n",
       "   7270,\n",
       "   22961,\n",
       "   1528,\n",
       "   1840,\n",
       "   1106,\n",
       "   21423,\n",
       "   1418,\n",
       "   2495,\n",
       "   12913,\n",
       "   119,\n",
       "   102],\n",
       "  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]},\n",
       " {'input_ids': [101, 1943, 14428, 102],\n",
       "  'attention_mask': [1, 1, 1, 1],\n",
       "  'labels': [-100, 1, 2, -100]}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenized_datasets['train'][i] for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9def3150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7270, 22961,  1528,  1840,  1106, 21423,  1418,  2495, 12913,\n",
       "           119,   102],\n",
       "        [  101,  1943, 14428,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n",
       "        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "batch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4661d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sealion\\AppData\\Local\\Temp/ipykernel_26704/1451933279.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('seqeval')\n"
     ]
    }
   ],
   "source": [
    "# !pip install seqeval \n",
    "from datasets import load_metric\n",
    "metric = load_metric('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "caecbee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MISC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'ORG': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'overall_precision': 0.5,\n",
       " 'overall_recall': 0.5,\n",
       " 'overall_f1': 0.5,\n",
       " 'overall_accuracy': 0.75}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=[['O', 'O', 'I-ORG', 'B-MISC']],\n",
    "              references=[['O', 'B-ORG', 'I-ORG', 'B-MISC']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e3d9527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    str_labels = [[label_names[t] for t in label if t != -100] for label in labels]\n",
    "    \n",
    "    str_preds=[\n",
    "        [label_names[p] for p, t in zip(pred, targ) if t != -100]\n",
    "        for pred, targ in zip(preds, labels)\n",
    "    ]\n",
    "    \n",
    "    the_metrics = metric.compute(predictions=str_preds, references=str_labels)\n",
    "    return {\n",
    "        'precision': the_metrics['overall_precision'],\n",
    "        'recall': the_metrics['overall_recall'],\n",
    "        'f1': the_metrics['overall_f1'],\n",
    "        'accuracy': the_metrics['overall_accuracy'],        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fb09d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label={k:v for k, v in enumerate(label_names)}\n",
    "label2id={v:k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18f98a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "checkpoint = \"distilbert-base-cased\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b05b9a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.scriptrun = azureml.core.script_run:ScriptRun._from_run_dto with exception (pywin32 228 (c:\\users\\sealion\\anaconda3\\lib\\site-packages), Requirement.parse('pywin32==227; sys_platform == \"win32\"'), {'docker'}).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "_n_gpu=1,\n",
       "adafactor=False,\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "bf16=False,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_pin_memory=True,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_steps=None,\n",
       "evaluation_strategy=epoch,\n",
       "fp16=False,\n",
       "fp16_backend=auto,\n",
       "fp16_full_eval=False,\n",
       "fp16_opt_level=O1,\n",
       "fsdp=[],\n",
       "fsdp_min_num_params=0,\n",
       "fsdp_transformer_layer_cls_to_wrap=None,\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=1,\n",
       "gradient_checkpointing=False,\n",
       "greater_is_better=None,\n",
       "group_by_length=False,\n",
       "half_precision_backend=auto,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=False,\n",
       "hub_strategy=every_save,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_inputs_for_metrics=False,\n",
       "jit_mode_eval=False,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=2e-05,\n",
       "length_column_name=length,\n",
       "load_best_model_at_end=False,\n",
       "local_rank=-1,\n",
       "log_level=passive,\n",
       "log_level_replica=passive,\n",
       "log_on_each_node=True,\n",
       "logging_dir=distilbert-finetuned-ner\\runs\\Aug04_14-19-21_DESKTOP-QDU5RTS,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=500,\n",
       "logging_strategy=steps,\n",
       "lr_scheduler_type=linear,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=None,\n",
       "mp_parameters=,\n",
       "no_cuda=False,\n",
       "num_train_epochs=3,\n",
       "optim=adamw_hf,\n",
       "optim_args=None,\n",
       "output_dir=distilbert-finetuned-ner,\n",
       "overwrite_output_dir=False,\n",
       "past_index=-1,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=8,\n",
       "prediction_loss_only=False,\n",
       "push_to_hub=False,\n",
       "push_to_hub_model_id=None,\n",
       "push_to_hub_organization=None,\n",
       "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       "ray_scope=last,\n",
       "remove_unused_columns=True,\n",
       "report_to=['mlflow', 'tensorboard'],\n",
       "resume_from_checkpoint=None,\n",
       "run_name=distilbert-finetuned-ner,\n",
       "save_on_each_node=False,\n",
       "save_steps=500,\n",
       "save_strategy=epoch,\n",
       "save_total_limit=None,\n",
       "seed=42,\n",
       "sharded_ddp=[],\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torchdynamo=None,\n",
       "tpu_metrics_debug=False,\n",
       "tpu_num_cores=None,\n",
       "use_ipex=False,\n",
       "use_legacy_prediction_loop=False,\n",
       "use_mps_device=False,\n",
       "warmup_ratio=0.0,\n",
       "warmup_steps=0,\n",
       "weight_decay=0.01,\n",
       "xpu_backend=None,\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    \"distilbert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d566d01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 14041\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5268\n",
      "  Number of trainable parameters = 65197833\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5268/5268 06:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.090033</td>\n",
       "      <td>0.890897</td>\n",
       "      <td>0.920734</td>\n",
       "      <td>0.905570</td>\n",
       "      <td>0.978645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.076998</td>\n",
       "      <td>0.916585</td>\n",
       "      <td>0.939414</td>\n",
       "      <td>0.927859</td>\n",
       "      <td>0.983826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.080032</td>\n",
       "      <td>0.921122</td>\n",
       "      <td>0.939414</td>\n",
       "      <td>0.930178</td>\n",
       "      <td>0.983723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-finetuned-ner\\checkpoint-1756\n",
      "Configuration saved in distilbert-finetuned-ner\\checkpoint-1756\\config.json\n",
      "Model weights saved in distilbert-finetuned-ner\\checkpoint-1756\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-finetuned-ner\\checkpoint-1756\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-finetuned-ner\\checkpoint-1756\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-finetuned-ner\\checkpoint-3512\n",
      "Configuration saved in distilbert-finetuned-ner\\checkpoint-3512\\config.json\n",
      "Model weights saved in distilbert-finetuned-ner\\checkpoint-3512\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-finetuned-ner\\checkpoint-3512\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-finetuned-ner\\checkpoint-3512\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-finetuned-ner\\checkpoint-5268\n",
      "Configuration saved in distilbert-finetuned-ner\\checkpoint-5268\\config.json\n",
      "Model weights saved in distilbert-finetuned-ner\\checkpoint-5268\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-finetuned-ner\\checkpoint-5268\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-finetuned-ner\\checkpoint-5268\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5268, training_loss=0.030547335461042283, metrics={'train_runtime': 387.4245, 'train_samples_per_second': 108.726, 'train_steps_per_second': 13.597, 'total_flos': 462023079274890.0, 'train_loss': 0.030547335461042283, 'epoch': 3.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "     model=model,\n",
    "     args=training_args,\n",
    "     train_dataset=tokenized_datasets['train'],\n",
    "     eval_dataset=tokenized_datasets['validation'],\n",
    "     data_collator=data_collator,\n",
    "     compute_metrics=compute_metrics,\n",
    "     tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cdbb5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to my_saved_model\n",
      "Configuration saved in my_saved_model\\config.json\n",
      "Model weights saved in my_saved_model\\pytorch_model.bin\n",
      "tokenizer config file saved in my_saved_model\\tokenizer_config.json\n",
      "Special tokens file saved in my_saved_model\\special_tokens_map.json\n",
      "loading configuration file my_saved_model\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"my_saved_model\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-PER\",\n",
      "    \"2\": \"I-PER\",\n",
      "    \"3\": \"B-ORG\",\n",
      "    \"4\": \"I-ORG\",\n",
      "    \"5\": \"B-LOC\",\n",
      "    \"6\": \"I-LOC\",\n",
      "    \"7\": \"B-MISC\",\n",
      "    \"8\": \"I-MISC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 5,\n",
      "    \"B-MISC\": 7,\n",
      "    \"B-ORG\": 3,\n",
      "    \"B-PER\": 1,\n",
      "    \"I-LOC\": 6,\n",
      "    \"I-MISC\": 8,\n",
      "    \"I-ORG\": 4,\n",
      "    \"I-PER\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file my_saved_model\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"my_saved_model\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-PER\",\n",
      "    \"2\": \"I-PER\",\n",
      "    \"3\": \"B-ORG\",\n",
      "    \"4\": \"I-ORG\",\n",
      "    \"5\": \"B-LOC\",\n",
      "    \"6\": \"I-LOC\",\n",
      "    \"7\": \"B-MISC\",\n",
      "    \"8\": \"I-MISC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 5,\n",
      "    \"B-MISC\": 7,\n",
      "    \"B-ORG\": 3,\n",
      "    \"B-PER\": 1,\n",
      "    \"I-LOC\": 6,\n",
      "    \"I-MISC\": 8,\n",
      "    \"I-ORG\": 4,\n",
      "    \"I-PER\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file my_saved_model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForTokenClassification.\n",
      "\n",
      "All the weights of DistilBertForTokenClassification were initialized from the model checkpoint at my_saved_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9997233,\n",
       "  'word': 'Bill Gates',\n",
       "  'start': 0,\n",
       "  'end': 10},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9989698,\n",
       "  'word': 'Microsoft',\n",
       "  'start': 26,\n",
       "  'end': 35},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9994497,\n",
       "  'word': 'Seattle',\n",
       "  'start': 39,\n",
       "  'end': 46},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99881625,\n",
       "  'word': 'Washington',\n",
       "  'start': 48,\n",
       "  'end': 58}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model('my_saved_model')\n",
    "from transformers import pipeline\n",
    "ner = pipeline(\n",
    "   \"token-classification\",\n",
    "    model=\"my_saved_model\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=0,)\n",
    "ner('Bill Gates was the CEO of Microsoft in Seattle, Washington.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9f3e81",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1235bc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Sealion\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Sealion\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')], [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('Executive', 'ADJ'), ('Committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('``', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('of', 'ADP'), ('Atlanta', 'NOUN'), (\"''\", '.'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install transformers datasets\n",
    "# !pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "corpus = brown.tagged_sents(tagset='universal')\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc746717",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "for sentence_tag_pairs in corpus:\n",
    "    tokens = []\n",
    "    target = []\n",
    "    for token, tag in sentence_tag_pairs:\n",
    "        tokens.append(token)\n",
    "        target.append(tag)\n",
    "    inputs.append(tokens)\n",
    "    targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3f4eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data.json', 'w') as f:\n",
    "    for x,y in zip(inputs, targets):\n",
    "        j = {'inputs':x, 'targets':y}\n",
    "        s = json.dumps(j)\n",
    "        f.write(f'{s}\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee6cc9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:/Users/Sealion/.cache/huggingface/datasets/json/default-9ec1cf0132016143/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6c80b05be049c790775efe9493c989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efde47f094a94337a1554e6bf9661259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/Sealion/.cache/huggingface/datasets/json/default-9ec1cf0132016143/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0f37ce247e4ea48c0f9d2eaa131250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 57340\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset('json', data_files='data.json')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95bf568d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['inputs', 'targets'],\n",
       "    num_rows: 20000\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small = data['train'].shuffle(seed=42).select(range(20_000))\n",
    "small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2e49551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': ['Ulyate',\n",
       "  'and',\n",
       "  'Kearton',\n",
       "  'climbed',\n",
       "  'on',\n",
       "  'toward',\n",
       "  'the',\n",
       "  'sound',\n",
       "  'of',\n",
       "  'the',\n",
       "  'barking',\n",
       "  'of',\n",
       "  'the',\n",
       "  'dogs',\n",
       "  'and',\n",
       "  'the',\n",
       "  'sporadic',\n",
       "  'roaring',\n",
       "  'of',\n",
       "  'the',\n",
       "  'lion',\n",
       "  ',',\n",
       "  'till',\n",
       "  'they',\n",
       "  'came',\n",
       "  ',',\n",
       "  'out',\n",
       "  'of',\n",
       "  'breath',\n",
       "  ',',\n",
       "  'to',\n",
       "  'the',\n",
       "  'crest',\n",
       "  ',',\n",
       "  'and',\n",
       "  'peering',\n",
       "  'through',\n",
       "  'the',\n",
       "  'branches',\n",
       "  'of',\n",
       "  'a',\n",
       "  'bush',\n",
       "  ',',\n",
       "  'this',\n",
       "  'is',\n",
       "  'what',\n",
       "  'Ulyate',\n",
       "  'saw',\n",
       "  ':',\n",
       "  'Jones',\n",
       "  'who',\n",
       "  'had',\n",
       "  'apparently',\n",
       "  '(',\n",
       "  'and',\n",
       "  'actually',\n",
       "  'had',\n",
       "  ')',\n",
       "  'ridden',\n",
       "  'up',\n",
       "  'the',\n",
       "  'nearly',\n",
       "  'impassable',\n",
       "  'hillside',\n",
       "  ',',\n",
       "  'sitting',\n",
       "  'calmly',\n",
       "  'on',\n",
       "  'his',\n",
       "  'horse',\n",
       "  'within',\n",
       "  'forty',\n",
       "  'feet',\n",
       "  'of',\n",
       "  'a',\n",
       "  'full-grown',\n",
       "  'young',\n",
       "  'lioness',\n",
       "  ',',\n",
       "  'who',\n",
       "  'was',\n",
       "  'crouched',\n",
       "  'on',\n",
       "  'a',\n",
       "  'flat',\n",
       "  'rock',\n",
       "  'and',\n",
       "  'seemed',\n",
       "  'just',\n",
       "  'about',\n",
       "  'to',\n",
       "  'charge',\n",
       "  'him',\n",
       "  ',',\n",
       "  'while',\n",
       "  'the',\n",
       "  'dogs',\n",
       "  'whirled',\n",
       "  'around',\n",
       "  'her',\n",
       "  '.'],\n",
       " 'targets': ['NOUN',\n",
       "  'CONJ',\n",
       "  'NOUN',\n",
       "  'VERB',\n",
       "  'PRT',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'CONJ',\n",
       "  'DET',\n",
       "  'ADJ',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  'ADP',\n",
       "  'PRON',\n",
       "  'VERB',\n",
       "  '.',\n",
       "  'PRT',\n",
       "  'ADP',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  'CONJ',\n",
       "  'VERB',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  'DET',\n",
       "  'VERB',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'VERB',\n",
       "  '.',\n",
       "  'NOUN',\n",
       "  'PRON',\n",
       "  'VERB',\n",
       "  'ADV',\n",
       "  '.',\n",
       "  'CONJ',\n",
       "  'ADV',\n",
       "  'VERB',\n",
       "  '.',\n",
       "  'VERB',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'ADV',\n",
       "  'ADJ',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  'VERB',\n",
       "  'ADV',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'NUM',\n",
       "  'NOUN',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'ADJ',\n",
       "  'ADJ',\n",
       "  'NOUN',\n",
       "  '.',\n",
       "  'PRON',\n",
       "  'VERB',\n",
       "  'VERB',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'ADJ',\n",
       "  'NOUN',\n",
       "  'CONJ',\n",
       "  'VERB',\n",
       "  'ADV',\n",
       "  'ADV',\n",
       "  'PRT',\n",
       "  'VERB',\n",
       "  'PRON',\n",
       "  '.',\n",
       "  'ADP',\n",
       "  'DET',\n",
       "  'NOUN',\n",
       "  'VERB',\n",
       "  'ADP',\n",
       "  'PRON',\n",
       "  '.']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = small.train_test_split(seed=42)\n",
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9681437d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'targets': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "abe781f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map tagets to ints\n",
    "target_set = set()\n",
    "for target in targets:\n",
    "    target_set = target_set.union(target)\n",
    "target_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "989948d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = list(target_set)\n",
    "id2label = {k:v for k,v in enumerate(target_list)}\n",
    "label2id = {v:k for k,v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "31547448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Sealion/.cache\\huggingface\\hub\\models--distilbert-base-cased\\snapshots\\4dc145c5bd4fdb672dcded7fdc1efd6c2bc55992\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\Sealion/.cache\\huggingface\\hub\\models--distilbert-base-cased\\snapshots\\4dc145c5bd4fdb672dcded7fdc1efd6c2bc55992\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\Sealion/.cache\\huggingface\\hub\\models--distilbert-base-cased\\snapshots\\4dc145c5bd4fdb672dcded7fdc1efd6c2bc55992\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Sealion/.cache\\huggingface\\hub\\models--distilbert-base-cased\\snapshots\\4dc145c5bd4fdb672dcded7fdc1efd6c2bc55992\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\Sealion/.cache\\huggingface\\hub\\models--distilbert-base-cased\\snapshots\\4dc145c5bd4fdb672dcded7fdc1efd6c2bc55992\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer   # bert\n",
    "checkpoint = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "faa5e7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ulyate',\n",
       " 'and',\n",
       " 'Kearton',\n",
       " 'climbed',\n",
       " 'on',\n",
       " 'toward',\n",
       " 'the',\n",
       " 'sound',\n",
       " 'of',\n",
       " 'the']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][idx]['inputs'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b83a79ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 158, 25928, 1566, 1105, 26835, 9349, 1320, 5998, 1113, 1755, 1103, 1839, 1104, 1103, 26635, 1104, 1103, 6363, 1105, 1103, 188, 27695, 23041, 1104, 1103, 11160, 117, 6174, 1152, 1338, 117, 1149, 1104, 2184, 117, 1106, 1103, 13468, 117, 1105, 19205, 1194, 1103, 5020, 1104, 170, 13771, 117, 1142, 1110, 1184, 158, 25928, 1566, 1486, 131, 2690, 1150, 1125, 4547, 113, 1105, 2140, 1125, 114, 17698, 1146, 1103, 2212, 24034, 11192, 1895, 25068, 117, 2807, 13285, 1113, 1117, 3241, 1439, 5808, 1623, 1104, 170, 1554, 118, 4215, 1685, 11160, 5800, 117, 1150, 1108, 15062, 1113, 170, 3596, 2067, 1105, 1882, 1198, 1164, 1106, 2965, 1140, 117, 1229, 1103, 6363, 18370, 1213, 1123, 119, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "t = tokenizer(data['train'][idx]['inputs'], is_split_into_words=True)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "01d49d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'U',\n",
       " '##lya',\n",
       " '##te',\n",
       " 'and',\n",
       " 'Ke',\n",
       " '##art',\n",
       " '##on',\n",
       " 'climbed',\n",
       " 'on',\n",
       " 'toward',\n",
       " 'the',\n",
       " 'sound',\n",
       " 'of',\n",
       " 'the',\n",
       " 'barking',\n",
       " 'of',\n",
       " 'the',\n",
       " 'dogs',\n",
       " 'and',\n",
       " 'the',\n",
       " 's',\n",
       " '##poradic',\n",
       " 'roaring',\n",
       " 'of',\n",
       " 'the',\n",
       " 'lion',\n",
       " ',',\n",
       " 'till',\n",
       " 'they',\n",
       " 'came',\n",
       " ',',\n",
       " 'out',\n",
       " 'of',\n",
       " 'breath',\n",
       " ',',\n",
       " 'to',\n",
       " 'the',\n",
       " 'crest',\n",
       " ',',\n",
       " 'and',\n",
       " 'peering',\n",
       " 'through',\n",
       " 'the',\n",
       " 'branches',\n",
       " 'of',\n",
       " 'a',\n",
       " 'bush',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'what',\n",
       " 'U',\n",
       " '##lya',\n",
       " '##te',\n",
       " 'saw',\n",
       " ':',\n",
       " 'Jones',\n",
       " 'who',\n",
       " 'had',\n",
       " 'apparently',\n",
       " '(',\n",
       " 'and',\n",
       " 'actually',\n",
       " 'had',\n",
       " ')',\n",
       " 'ridden',\n",
       " 'up',\n",
       " 'the',\n",
       " 'nearly',\n",
       " 'imp',\n",
       " '##ass',\n",
       " '##able',\n",
       " 'hillside',\n",
       " ',',\n",
       " 'sitting',\n",
       " 'calmly',\n",
       " 'on',\n",
       " 'his',\n",
       " 'horse',\n",
       " 'within',\n",
       " 'forty',\n",
       " 'feet',\n",
       " 'of',\n",
       " 'a',\n",
       " 'full',\n",
       " '-',\n",
       " 'grown',\n",
       " 'young',\n",
       " 'lion',\n",
       " '##ess',\n",
       " ',',\n",
       " 'who',\n",
       " 'was',\n",
       " 'crouched',\n",
       " 'on',\n",
       " 'a',\n",
       " 'flat',\n",
       " 'rock',\n",
       " 'and',\n",
       " 'seemed',\n",
       " 'just',\n",
       " 'about',\n",
       " 'to',\n",
       " 'charge',\n",
       " 'him',\n",
       " ',',\n",
       " 'while',\n",
       " 'the',\n",
       " 'dogs',\n",
       " 'whirled',\n",
       " 'around',\n",
       " 'her',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(t))\n",
    "t.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "710d386c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 0, 0, 1, 2, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_ids()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aa066da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100, 1, 7, 1, 2, 8, 3, 4, 1, 1, 3, -100]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def align_targets(labels, word_ids):    \n",
    "    aligned_labels = []\n",
    "    for word in word_ids:\n",
    "        if word is None: \n",
    "            label = -100\n",
    "        else:\n",
    "            label = label2id[labels[word]]\n",
    "        aligned_labels.append(label)\n",
    "    return aligned_labels\n",
    "\n",
    "labels = data['train'][idx]['targets']\n",
    "word_idx = t.word_ids()\n",
    "aligned_targets = align_targets(labels, word_ids)\n",
    "aligned_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9cfdf4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN', 'CONJ', 'NOUN', 'VERB', 'PRT', 'ADP', 'DET', 'NOUN', 'ADP', 'DET']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][idx]['targets'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aba0dafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\tNone\n",
      "U\tNOUN\n",
      "##lya\tCONJ\n",
      "##te\tNOUN\n",
      "and\tVERB\n",
      "Ke\tPRT\n",
      "##art\tADP\n",
      "##on\tDET\n",
      "climbed\tNOUN\n",
      "on\tNOUN\n",
      "toward\tADP\n",
      "the\tNone\n"
     ]
    }
   ],
   "source": [
    "aligned_labels = [id2label[i] if i >= 0 else None for i in aligned_targets]\n",
    "for x,y in zip(t.tokens(), aligned_labels):\n",
    "    print(f\"{x}\\t{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3b4ab8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize both inputs and targets\n",
    "def tokenize_fn(batch):\n",
    "    tokenized_inputs = tokenizer(batch['inputs'], truncation=True, is_split_into_words=True)\n",
    "    labels_batch = batch['targets']\n",
    "    aligned_labels_batch = []\n",
    "    for i, labels in enumerate(labels_batch):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        aligned_labels_batch.append(align_targets(labels,word_ids))\n",
    "        tokenized_inputs['labels'] = aligned_labels_batch\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = data.map(tokenize_fn, batched=True, remove_columns=data['train'].column_names,)\n",
    "tokenized_datasets        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2440a381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 7,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 9,\n",
       " 8,\n",
       " 3,\n",
       " 1,\n",
       " 9,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 9,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 10,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 11,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 2,\n",
       " 5,\n",
       " 9,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " -100]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "46093601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    flattened = [val for sublist in list_of_lists for val in sublist]\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2a29d934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForTokenClassification(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aaeeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    preds = np.argmax(logitx,axis=-1)\n",
    "    labels_jagged = [[t for t in nlabel if t != -100] for label in labels]\n",
    "    preds_jaggles = [[p for p, t in zip(ps,ts) if t != -100] for ps, ts in zip(preds, labels)]\n",
    "\n",
    "    labels_flat = flatten(labels_jagged)\n",
    "    preds_flat = flatten(preds_jagged)\n",
    "\n",
    "    acc = accuracy_score(labels_flat, preds_flat)\n",
    "    f1 = f1_score(labels_flat, preds_flat, average='macro')\n",
    "    \n",
    "    return {'f1':f1, 'accuracy':acc,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a72d724d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Sealion/.cache\\huggingface\\hub\\models--distilbert-base-cased\\snapshots\\4dc145c5bd4fdb672dcded7fdc1efd6c2bc55992\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"ADJ\",\n",
      "    \"1\": \"NOUN\",\n",
      "    \"2\": \"VERB\",\n",
      "    \"3\": \"ADP\",\n",
      "    \"4\": \"DET\",\n",
      "    \"5\": \"PRON\",\n",
      "    \"6\": \"X\",\n",
      "    \"7\": \"CONJ\",\n",
      "    \"8\": \"PRT\",\n",
      "    \"9\": \".\",\n",
      "    \"10\": \"ADV\",\n",
      "    \"11\": \"NUM\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \".\": 9,\n",
      "    \"ADJ\": 0,\n",
      "    \"ADP\": 3,\n",
      "    \"ADV\": 10,\n",
      "    \"CONJ\": 7,\n",
      "    \"DET\": 4,\n",
      "    \"NOUN\": 1,\n",
      "    \"NUM\": 11,\n",
      "    \"PRON\": 5,\n",
      "    \"PRT\": 8,\n",
      "    \"VERB\": 2,\n",
      "    \"X\": 6\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Sealion/.cache\\huggingface\\hub\\models--distilbert-base-cased\\snapshots\\4dc145c5bd4fdb672dcded7fdc1efd6c2bc55992\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Sealion\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 15000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5625\n",
      "  Number of trainable parameters = 65200140\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1876' max='5625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1876/5625 01:44 < 03:28, 17.98 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26704/82251136.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m                  \u001b[0mcompute_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                  tokenizer=tokenizer,)\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1541\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m         )\n\u001b[1;32m-> 1543\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1544\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1545\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mDebugOption\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTPU_METRICS_DEBUG\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2129\u001b[0m                     )\n\u001b[0;32m   2130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2131\u001b[1;33m                 \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2132\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2826\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2827\u001b[1;33m         output = eval_loop(\n\u001b[0m\u001b[0;32m   2828\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2829\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Evaluation\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3114\u001b[0m                 )\n\u001b[0;32m   3115\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3116\u001b[1;33m                 \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvalPrediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3117\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3118\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26704/2326803634.py\u001b[0m in \u001b[0;36mcompute_metrics\u001b[1;34m(logits_and_labels)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mstr_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     str_preds=[\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26704/2326803634.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mstr_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     str_preds=[\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26704/2326803634.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mstr_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     str_preds=[\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"distilbert-base-cased\", id2label=id2label, label2id=label2id,)\n",
    "trainer = Trainer(model=model,\n",
    "                 args=training_args,\n",
    "                 train_dataset=tokenized_datasets['train'],\n",
    "                 eval_dataset=tokenized_datasets['test'],\n",
    "                 data_collator=data_collator,\n",
    "                 compute_metrics=compute_metrics,\n",
    "                 tokenizer=tokenizer,)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "481fb9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to my_saved_model\n",
      "Configuration saved in my_saved_model\\config.json\n",
      "Model weights saved in my_saved_model\\pytorch_model.bin\n",
      "tokenizer config file saved in my_saved_model\\tokenizer_config.json\n",
      "Special tokens file saved in my_saved_model\\special_tokens_map.json\n",
      "loading configuration file my_saved_model\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"my_saved_model\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"ADJ\",\n",
      "    \"1\": \"NOUN\",\n",
      "    \"2\": \"VERB\",\n",
      "    \"3\": \"ADP\",\n",
      "    \"4\": \"DET\",\n",
      "    \"5\": \"PRON\",\n",
      "    \"6\": \"X\",\n",
      "    \"7\": \"CONJ\",\n",
      "    \"8\": \"PRT\",\n",
      "    \"9\": \".\",\n",
      "    \"10\": \"ADV\",\n",
      "    \"11\": \"NUM\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \".\": 9,\n",
      "    \"ADJ\": 0,\n",
      "    \"ADP\": 3,\n",
      "    \"ADV\": 10,\n",
      "    \"CONJ\": 7,\n",
      "    \"DET\": 4,\n",
      "    \"NOUN\": 1,\n",
      "    \"NUM\": 11,\n",
      "    \"PRON\": 5,\n",
      "    \"PRT\": 8,\n",
      "    \"VERB\": 2,\n",
      "    \"X\": 6\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file my_saved_model\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"my_saved_model\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"ADJ\",\n",
      "    \"1\": \"NOUN\",\n",
      "    \"2\": \"VERB\",\n",
      "    \"3\": \"ADP\",\n",
      "    \"4\": \"DET\",\n",
      "    \"5\": \"PRON\",\n",
      "    \"6\": \"X\",\n",
      "    \"7\": \"CONJ\",\n",
      "    \"8\": \"PRT\",\n",
      "    \"9\": \".\",\n",
      "    \"10\": \"ADV\",\n",
      "    \"11\": \"NUM\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \".\": 9,\n",
      "    \"ADJ\": 0,\n",
      "    \"ADP\": 3,\n",
      "    \"ADV\": 10,\n",
      "    \"CONJ\": 7,\n",
      "    \"DET\": 4,\n",
      "    \"NOUN\": 1,\n",
      "    \"NUM\": 11,\n",
      "    \"PRON\": 5,\n",
      "    \"PRT\": 8,\n",
      "    \"VERB\": 2,\n",
      "    \"X\": 6\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file my_saved_model\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForTokenClassification.\n",
      "\n",
      "All the weights of DistilBertForTokenClassification were initialized from the model checkpoint at my_saved_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('my_saved_model')\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\n",
    "    'token-classification',\n",
    "     model='my_saved_model',\n",
    "     device=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "07c7d963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'NOUN',\n",
       "  'score': 0.9982545,\n",
       "  'index': 1,\n",
       "  'word': 'Bill',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.99915516,\n",
       "  'index': 2,\n",
       "  'word': 'Gates',\n",
       "  'start': 5,\n",
       "  'end': 10},\n",
       " {'entity': 'VERB',\n",
       "  'score': 0.9996705,\n",
       "  'index': 3,\n",
       "  'word': 'was',\n",
       "  'start': 11,\n",
       "  'end': 14},\n",
       " {'entity': 'DET',\n",
       "  'score': 0.99968576,\n",
       "  'index': 4,\n",
       "  'word': 'the',\n",
       "  'start': 15,\n",
       "  'end': 18},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.9986363,\n",
       "  'index': 5,\n",
       "  'word': 'CEO',\n",
       "  'start': 19,\n",
       "  'end': 22},\n",
       " {'entity': 'ADP',\n",
       "  'score': 0.999655,\n",
       "  'index': 6,\n",
       "  'word': 'of',\n",
       "  'start': 23,\n",
       "  'end': 25},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.9978656,\n",
       "  'index': 7,\n",
       "  'word': 'Microsoft',\n",
       "  'start': 26,\n",
       "  'end': 35},\n",
       " {'entity': 'ADP',\n",
       "  'score': 0.9994708,\n",
       "  'index': 8,\n",
       "  'word': 'in',\n",
       "  'start': 36,\n",
       "  'end': 38},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.9992939,\n",
       "  'index': 9,\n",
       "  'word': 'Seattle',\n",
       "  'start': 39,\n",
       "  'end': 46},\n",
       " {'entity': '.',\n",
       "  'score': 0.9997249,\n",
       "  'index': 10,\n",
       "  'word': ',',\n",
       "  'start': 46,\n",
       "  'end': 47},\n",
       " {'entity': 'NOUN',\n",
       "  'score': 0.99930334,\n",
       "  'index': 11,\n",
       "  'word': 'Washington',\n",
       "  'start': 48,\n",
       "  'end': 58},\n",
       " {'entity': '.',\n",
       "  'score': 0.99981326,\n",
       "  'index': 12,\n",
       "  'word': '.',\n",
       "  'start': 58,\n",
       "  'end': 59}]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe('Bill Gates was the CEO of Microsoft in Seattle, Washington.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece0283",
   "metadata": {},
   "source": [
    "#### Acknowledges:\n",
    "\n",
    "1. Dataset: NER/POS tags datasets\n",
    "2. Courses of Lazy Programmer includes practical exercise.\n",
    "All above is for practice purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beaa0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf18ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b94cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
