{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression and Naive Bayes for NLP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Sealion\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sealion\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from os import getcwd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import twitter_samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and    \n",
    "                word not in string.punctuation):  \n",
    "            stem_word = stemmer.stem(word)       \n",
    "            tweets_clean.append(stem_word)\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    # Convert np array to list since zip.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "print(len(all_positive_tweets), len(all_negative_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (7000, 1)\n",
      "test_y.shape = (3000, 1)\n"
     ]
    }
   ],
   "source": [
    "test_pos = all_positive_tweets[3500:]\n",
    "train_pos = all_positive_tweets[:3500]\n",
    "test_neg = all_negative_tweets[3500:]\n",
    "train_neg = all_negative_tweets[:3500]\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))\n",
    "freqs = build_freqs(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1). Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    h = 1 / (1 + np.exp(-z))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    m = x.shape[0]    \n",
    "    for i in range(0, num_iters):\n",
    "        z = np.dot(x,theta)\n",
    "        h = sigmoid(z)        \n",
    "        # calculate the cost function\n",
    "        J = -1./m * (np.dot(y.transpose(), np.log(h)) + np.dot((1-y).transpose(),np.log(1-h)))    \n",
    "        # update the weights theta\n",
    "        theta = theta = theta - (alpha/m) * np.dot(x.transpose(),(h-y))\n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    x = np.zeros((1, 3))   \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "    for word in word_l:        \n",
    "        # count for the positive label 1\n",
    "        x[0,1] += freqs.get((word, 1.0),0)        \n",
    "        # count for the negative label 0\n",
    "        x[0,2] += freqs.get((word, 0.0),0)\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after training is 0.33243085.\n",
      "The resulting vector of weights is [4e-08, 0.00037801, -0.00044195]\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "Y = train_y\n",
    "# Apply gradient descent\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1000)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    x = extract_features(tweet,freqs)    \n",
    "    y_pred = sigmoid(np.dot(x,theta))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    y_hat = []    \n",
    "    for tweet in test_x:\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)        \n",
    "        if y_pred > 0.5:\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            y_hat.append(0)\n",
    "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression's accuracy = 0.9910\n"
     ]
    }
   ],
   "source": [
    "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression's accuracy = {tmp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Predicted Tweet\n",
      "THE TWEET IS: Dare I say that there is a video of me on our About Page - http://t.co/1nXM8mxmbu - talking infrared heating :-)\n",
      "THE PROCESSED TWEET IS: ['dare', 'say', 'video', 'page']\n",
      "1\t0.49932441\tb'dare say video page'\n",
      "THE TWEET IS: @Th0tiana_ fruity vodkas always help : )\n",
      "THE PROCESSED TWEET IS: ['fruiti', 'vodka', 'alway', 'help']\n",
      "1\t0.49835917\tb'fruiti vodka alway help'\n",
      "THE TWEET IS: Where's the time going?! ONLY 40 days to go &amp; sooo much to do!! Not enough days in the week :p #goodmorning http://t.co/4NPwOGr9QL\n",
      "THE PROCESSED TWEET IS: [\"where'\", 'time', 'go', '40', 'day', 'go', 'sooo', 'much', 'enough', 'day', 'week', ':p', 'goodmorn']\n",
      "1\t0.49456871\tb\"where' time go 40 day go sooo much enough day week :p goodmorn\"\n",
      "THE TWEET IS: I ATE YOUR LAST COOKIE SHIR0 &gt;:D\n",
      "THE PROCESSED TWEET IS: ['ate', 'last', 'cooki', 'shir', '0', '>:d']\n",
      "1\t0.49718284\tb'ate last cooki shir 0 >:d'\n",
      "THE TWEET IS: @hesaffection are you the owner of the user @hesIovely ? : )\n",
      "THE PROCESSED TWEET IS: ['owner', 'user']\n",
      "1\t0.49996804\tb'owner user'\n",
      "THE TWEET IS: @75Susann \n",
      "how goes it : ) : P\n",
      "THE PROCESSED TWEET IS: ['goe', 'p']\n",
      "1\t0.49992008\tb'goe p'\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
      "http://t.co/6TAEpQMBaN http://t.co/Le7bpWbMlO\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48762462\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: @F41rygirl @paintingandbook You saying you want Lucy to be gone soon, Lisa!!? Oh that's horrible!\n",
      "Sorry :-)  \n",
      "Yes, be back soon, please!! x\n",
      "THE PROCESSED TWEET IS: ['say', 'want', 'luci', 'gone', 'soon', 'lisa', 'oh', \"that'\", 'horribl', 'sorri', ':-)', 'ye', 'back', 'soon', 'pleas', 'x']\n",
      "1\t0.49097322\tb\"say want luci gone soon lisa oh that' horribl sorri :-) ye back soon pleas x\"\n",
      "THE TWEET IS: @planetjedward GoodMorning ! What's coming next? =:D =:D\n",
      "THE PROCESSED TWEET IS: ['goodmorn', \"what'\", 'come', 'next', '=:', '=:']\n",
      "1\t0.49822847\tb\"goodmorn what' come next =: =:\"\n",
      "THE TWEET IS: @osehxn94 reserve with the password, okay? : )\n",
      "THE PROCESSED TWEET IS: ['reserv', 'password', 'okay']\n",
      "1\t0.49999438\tb'reserv password okay'\n",
      "THE TWEET IS: If you wanna see you more beautiful then rotate your photo !\n",
      "It works.....\n",
      ":p\n",
      ":v\n",
      "THE PROCESSED TWEET IS: ['wanna', 'see', 'beauti', 'rotat', 'photo', 'work', '...', ':p', 'v']\n",
      "1\t0.49895126\tb'wanna see beauti rotat photo work ... :p v'\n",
      "THE TWEET IS: @jaredNOTsubway @iluvmariah @Bravotv Then that truly is a LATERAL move! Now, we all know the Queen Bee is UPWARD BOUND : ) #MovingOnUp\n",
      "THE PROCESSED TWEET IS: ['truli', 'later', 'move', 'know', 'queen', 'bee', 'upward', 'bound', 'movingonup']\n",
      "1\t0.49902682\tb'truli later move know queen bee upward bound movingonup'\n",
      "THE TWEET IS: @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
      "THE PROCESSED TWEET IS: ['sure', 'would', 'good', 'thing', '4', 'bottom', 'dare', '2', 'say', '2', 'miss', 'b', 'im', 'gonna', 'stubborn', 'mouth', 'soap', 'nothavingit', ':p']\n",
      "1\t0.48651450\tb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
      "http://t.co/UGQzOx0huu\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48762462\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/aOKldo3GMj http://t.co/xWCM9qyRG5\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48762462\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: Movie 'Key of Life' (Japanese Version) https://t.co/ib4kfpbBu8 interesting storyline! :)\n",
      "THE PROCESSED TWEET IS: ['movi', 'key', 'life', 'japanes', 'version']\n",
      "1\t0.49907428\tb'movi key life japanes version'\n",
      "THE TWEET IS: A new report talks about how we burn more calories in the cold, because we work harder to warm up. Feel any better about the weather? :p\n",
      "THE PROCESSED TWEET IS: ['new', 'report', 'talk', 'burn', 'calori', 'cold', 'work', 'harder', 'warm', 'feel', 'better', 'weather', ':p']\n",
      "1\t0.49533296\tb'new report talk burn calori cold work harder warm feel better weather :p'\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/R2JBO8iNww http://t.co/ow5BBwdEMY\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48762462\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: @Mathpro314 Hey, wanna check out our YTB Channel? We post Gameplays &amp; Tutorials! https://t.co/sc9kDhaviX :) via http://t.co/J3sxzzg7cU\n",
      "THE PROCESSED TWEET IS: ['hey', 'wanna', 'check', 'ytb', 'channel', 'post', 'gameplay', 'tutori']\n",
      "1\t0.49983220\tb'hey wanna check ytb channel post gameplay tutori'\n",
      "THE TWEET IS: off to the park to get some sunlight : )\n",
      "THE PROCESSED TWEET IS: ['park', 'get', 'sunlight']\n",
      "1\t0.49540998\tb'park get sunlight'\n",
      "THE TWEET IS: @msarosh Uff Itna Miss karhy thy ap :p\n",
      "THE PROCESSED TWEET IS: ['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n",
      "1\t0.48707063\tb'uff itna miss karhi thi ap :p'\n",
      "THE TWEET IS: @kevinngmingyuan peasant seats to watch a peasant team...I don't mind :p ahahha\n",
      "THE PROCESSED TWEET IS: ['peasant', 'seat', 'watch', 'peasant', 'team', '...', 'mind', ':p', 'ahahha']\n",
      "1\t0.49879275\tb'peasant seat watch peasant team ... mind :p ahahha'\n",
      "THE TWEET IS: “@RobinhoodApp: We love spotting Robinhood out in the wild! Thanks for all the support out there, Robinhoodies! 🚙💚 http://t.co/bK6z2WhXMK”:(\n",
      "THE PROCESSED TWEET IS: ['“', 'love', 'spot', 'robinhood', 'wild', 'thank', 'support', 'robinhoodi', '🚙', '💚']\n",
      "0\t0.54772697\tb' love spot robinhood wild thank support robinhoodi  '\n",
      "THE TWEET IS: @Mickb1980 @CalderClarion @ev2cycling Looks good pal. Glad I paid £111 for my jersey and gilet! : (\n",
      "THE PROCESSED TWEET IS: ['look', 'good', 'pal', 'glad', 'paid', '£', '111', 'jersey', 'gilet']\n",
      "0\t0.51273595\tb'look good pal glad paid  111 jersey gilet'\n",
      "THE TWEET IS: Biodiversity, Taxonomic Infrastructure, International Collaboration, and New Species Discovery http://t.co/BWNMCNBvnC Suppl. data as PDF :(\n",
      "THE PROCESSED TWEET IS: ['biodivers', 'taxonom', 'infrastructur', 'intern', 'collabor', 'new', 'speci', 'discoveri']\n",
      "0\t0.50469918\tb'biodivers taxonom infrastructur intern collabor new speci discoveri'\n",
      "THE TWEET IS: pats jay : (\n",
      "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
      "0\t0.50018902\tb'pat jay'\n",
      "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
      "THE PROCESSED TWEET IS: ['belov', 'grandmoth']\n",
      "0\t0.50000001\tb'belov grandmoth'\n"
     ]
    }
   ],
   "source": [
    "print('Label Predicted Tweet')\n",
    "for x,y in zip(test_x,test_y):\n",
    "    y_hat = predict_tweet(x, freqs, theta)\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        print('THE TWEET IS:', x)\n",
    "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
    "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  (2). Naive Bayes models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sets of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "# split the data\n",
    "test_pos = all_positive_tweets[3500:]\n",
    "train_pos = all_positive_tweets[:3500]\n",
    "test_neg = all_negative_tweets[3500:]\n",
    "train_neg = all_negative_tweets[:3500]\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets(result, tweets, ys):\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word,y)\n",
    "            if pair in result:\n",
    "                result[pair] += 1\n",
    "            else:\n",
    "                result[pair] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "            N_pos += freqs[pair]\n",
    "        else:\n",
    "            N_neg += freqs[pair]\n",
    "    D = len(train_y)\n",
    "    D_pos = (len(list(filter(lambda x: x > 0, train_y))))\n",
    "    D_neg = (len(list(filter(lambda x: x <= 0, train_y))))\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "\n",
    "    for word in vocab:\n",
    "        freq_pos = lookup(freqs,word,1)\n",
    "        freq_neg = lookup(freqs,word,0)\n",
    "        # computer the probability\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(freqs, word, label):\n",
    "    n = 0  \n",
    "    pair = (word, label)\n",
    "    if (pair in freqs):\n",
    "        n = freqs[pair]\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "8304\n"
     ]
    }
   ],
   "source": [
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    word_l = process_tweet(tweet)\n",
    "    # initialize \n",
    "    p = 0\n",
    "    p += logprior\n",
    "    for word in word_l:\n",
    "        if word in loglikelihood:\n",
    "            p += loglikelihood[word]\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    accuracy = 0  \n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            y_hat_i = 0\n",
    "        y_hats.append(y_hat_i)\n",
    "    error = np.mean(np.absolute(y_hats-test_y))\n",
    "    accuracy = 1-error\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.9920\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth Predicted Tweet\n",
      "1\t0.00\tb'fruiti vodka alway help'\n",
      "1\t0.00\tb'ate last cooki shir 0 >:d'\n",
      "1\t0.00\tb\"say want luci gone soon lisa oh that' horribl sorri :-) ye back soon pleas x\"\n",
      "1\t0.00\tb''\n",
      "1\t0.00\tb''\n",
      "1\t0.00\tb'truli later move know queen bee upward bound movingonup'\n",
      "1\t0.00\tb'movi key life japanes version'\n",
      "1\t0.00\tb'new report talk burn calori cold work harder warm feel better weather :p'\n",
      "1\t0.00\tb'harri niall 94 harri born ik stupid wanna chang :D'\n",
      "1\t0.00\tb''\n",
      "1\t0.00\tb''\n",
      "1\t0.00\tb'park get sunlight'\n",
      "1\t0.00\tb'uff itna miss karhi thi ap :p'\n",
      "0\t1.00\tb'rohingya muslim 72 indict human traffick charg thailand asia around ...'\n",
      "0\t1.00\tb'screenshot'\n",
      "0\t1.00\tb' love spot robinhood wild thank support robinhoodi  '\n",
      "0\t1.00\tb'look good pal glad paid  111 jersey gilet'\n",
      "0\t1.00\tb'biodivers taxonom infrastructur intern collabor new speci discoveri'\n",
      "0\t1.00\tb'srsli fuck u unfollow hope ur futur child unpar u >:-('\n",
      "0\t1.00\tb'bianca ur one bun'\n",
      "0\t1.00\tb'hello info possibl interest jonatha close join beti :( great'\n",
      "0\t1.00\tb'u prob fun david'\n",
      "0\t1.00\tb'pat jay'\n",
      "0\t1.00\tb'whatev stil l young >:-('\n"
     ]
    }
   ],
   "source": [
    "# Error Analysis\n",
    "print('Truth Predicted Tweet')\n",
    "for x, y in zip(test_x, test_y):\n",
    "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
